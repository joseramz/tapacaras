<!doctype html>
<html lang="es">
<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width,initial-scale=1" />
  <title>Demo cámara + blur de caras</title>
  <style>
    * { box-sizing: border-box; }
    body {
      margin: 0;
      padding: 12px;
      font-family: system-ui, -apple-system, BlinkMacSystemFont, "Segoe UI", sans-serif;
      display: flex;
      flex-direction: column;
      gap: 12px;
      background: #f5f5f5;
    }
    h1 { font-size: 18px; margin: 0; }
    .layout { display: flex; flex-wrap: wrap; gap: 12px; }
    #wrap {
      position: relative;
      width: 360px;
      max-width: 100%;
      background: #000;
      border-radius: 8px;
      overflow: hidden;
    }
    video, canvas {
      width: 100%;
      height: auto;
      display: block;
    }
    /* El vídeo puede estar oculto, usamos el canvas como vista principal */
    video {
      display: none;
    }
    .controls {
      display: flex;
      flex-wrap: wrap;
      gap: 8px;
      align-items: center;
    }
    button, a.button-link {
      padding: 6px 10px;
      border-radius: 4px;
      border: 1px solid #ccc;
      background: #fff;
      cursor: pointer;
      font-size: 14px;
      text-decoration: none;
      color: #000;
    }
    button:disabled { opacity: 0.5; cursor: default; }
    .note { font-size: 12px; color: #666; }
  </style>
</head>
<body>
  <h1>Demo cámara + blur de caras</h1>
  <p class="note">
    1) Pulsa <strong>Abrir cámara</strong> · 2) Deberías verte en el recuadro con la cara desenfocada ·
    3) Pulsa <strong>Capturar</strong> para descargar la imagen procesada.
  </p>

  <div class="layout">
    <div id="wrap">
      <video id="video" autoplay playsinline muted></video>
      <canvas id="canvas"></canvas>
    </div>

    <div style="min-width:220px;max-width:300px;">
      <div class="controls">
        <button id="start">Abrir cámara</button>
        <button id="capture" disabled>Capturar</button>
      </div>
      <div class="controls">
        <a id="download" class="button-link" href="javascript:void(0);">Descargar (tras capturar)</a>
      </div>
      <p class="note">
        Todo se procesa en tu navegador. Si ves el vídeo pero no se desenfoca la cara,
        es que la parte de MediaPipe no ha cargado y solo estás viendo la cámara.
      </p>
    </div>
  </div>

  <!-- MediaPipe Face Detection (legacy JS API, global FaceDetection) -->
  <script src="https://cdn.jsdelivr.net/npm/@mediapipe/face_detection@0.4/face_detection.js"></script>

  <script>
    const video = document.getElementById('video');
    const canvas = document.getElementById('canvas');
    const ctx = canvas.getContext('2d');
    const startBtn = document.getElementById('start');
    const captureBtn = document.getElementById('capture');
    const downloadLink = document.getElementById('download');

    let stream = null;

    // MediaPipe
    let faceDetection = null;
    let lastDetections = null;
    let detectionTimer = null;
    let renderStarted = false;

    // Inicializa MediaPipe FaceDetection (si está disponible)
    function initFaceDetection() {
      if (faceDetection) return; // ya está
      if (typeof FaceDetection === 'undefined') {
        console.warn('MediaPipe FaceDetection no se ha cargado (script/ CDN). Solo se mostrará la cámara.');
        return;
      }

      faceDetection = new FaceDetection({
        locateFile: (file) =>
          `https://cdn.jsdelivr.net/npm/@mediapipe/face_detection@0.4/${file}`,
      });

      faceDetection.setOptions({
        model: 'short',              // modelo corto alcance
        minDetectionConfidence: 0.6, // umbral
      });

      faceDetection.onResults((results) => {
        lastDetections = results;
      });
    }

    // Loop de pintado: siempre dibuja el vídeo; si hay detecciones, aplica blur
    function renderLoop() {
      if (!video.videoWidth || !video.videoHeight) {
        requestAnimationFrame(renderLoop);
        return;
      }

      const vw = video.videoWidth;
      const vh = video.videoHeight;
      canvas.width = vw;
      canvas.height = vh;

      // 1) dibujar frame original
      ctx.drawImage(video, 0, 0, vw, vh);

      // 2) si hay detecciones, desenfocar regiones
      if (lastDetections && lastDetections.detections && lastDetections.detections.length) {
        for (const det of lastDetections.detections) {
          const box = det.boundingBox;
          const x = box.xCenter * vw - (box.width * vw) / 2;
          const y = box.yCenter * vh - (box.height * vh) / 2;
          const w = box.width * vw;
          const h = box.height * vh;
          blurRegion(x, y, w, h, 10);
        }
      }

      requestAnimationFrame(renderLoop);
    }

    // Blur sencillo por downscale/upscale
    function blurRegion(x, y, w, h, strength = 8) {
      const sx = Math.max(0, Math.floor(x));
      const sy = Math.max(0, Math.floor(y));
      const sw = Math.min(canvas.width - sx, Math.floor(w));
      const sh = Math.min(canvas.height - sy, Math.floor(h));
      if (sw <= 0 || sh <= 0) return;

      const tmp = document.createElement('canvas');
      tmp.width = Math.max(1, Math.floor(sw / strength));
      tmp.height = Math.max(1, Math.floor(sh / strength));
      const tctx = tmp.getContext('2d');

      tctx.drawImage(canvas, sx, sy, sw, sh, 0, 0, tmp.width, tmp.height);
      ctx.drawImage(tmp, 0, 0, tmp.width, tmp.height, sx, sy, sw, sh);
    }

    // Botón: abrir cámara
    startBtn.addEventListener('click', async () => {
      try {
        // Cámara (esto ya sabes que te funciona en la versión mínima)
        stream = await navigator.mediaDevices.getUserMedia({
          video: { facingMode: 'user' },
          audio: false,
        });
        video.srcObject = stream;
        await video.play();

        // Arrancar pintado continuo (vídeo -> canvas)
        if (!renderStarted) {
          renderStarted = true;
          requestAnimationFrame(renderLoop);
        }

        // Inicializar MediaPipe (si el script está ok)
        initFaceDetection();

        // Lanzar detección periódica (cada 500ms) si existe faceDetection
        if (faceDetection && !detectionTimer) {
          detectionTimer = setInterval(async () => {
            try {
              if (video.readyState >= 2) {
                await faceDetection.send({ image: video });
              }
            } catch (e) {
              console.error('Error en faceDetection.send:', e);
            }
          }, 500);
        }

        captureBtn.disabled = false;
      } catch (err) {
        console.error('Error al acceder a la cámara:', err);
        alert('No se pudo acceder a la cámara. Revisa permisos, HTTPS y ajustes del navegador.');
      }
    });

    // Botón: capturar → descargar frame actual del canvas (con blur aplicado si lo hay)
    captureBtn.addEventListener('click', () => {
      if (!canvas.width || !canvas.height) return;

      const data = canvas.toDataURL('image/png');
      downloadLink.href = data;
      downloadLink.download = 'captura-blur.png';
      downloadLink.textContent = 'Descargar captura';
    });

    window.addEventListener('beforeunload', () => {
      if (stream) {
        stream.getTracks().forEach(t => t.stop());
      }
      if (detectionTimer) clearInterval(detectionTimer);
    });
  </script>
</body>
</html>